{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7 Q4\n",
    "\n",
    "- The relatively low percentage of the model's explanatary power suggests that the model doesn't capture all factors that influence the dependent variable. In other words, there are some unobserved predictors.\n",
    "- For coefficients, the strong evidence against the null hypothesis suggests that the predictor variables have a meaningful association with the outcome. In other words, the predictors chosen are quite \"correct\".\n",
    "- This contradiction can exist because low $R^2$ and high p-value of coefficients are not mutually exclusive. We can conclude that we chose correct predictors but there are still some unobserved predictors when this contradiction happens.\n",
    "\n",
    "## GPT's summary\n",
    "- https://chatgpt.com/share/6736c926-c10c-8006-be94-a920534d8378"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7 Q7\n",
    "\n",
    "Principles of Model Extension\n",
    "- Model 3 & 4 to Model 5\n",
    "  - Model 3 is a simple linear regression with two predictors, `Attack` and `Defense`, predicting `HP`\n",
    "  - Model 4 expands Model 3 by adding more predictors  (e.g., `Speed`, `Legendary`, etc.)\n",
    "  - Model 5 incorporates categorical variables like `Generation`, `Type 1`, and `Type 2`. It's like adding the indicator variables to our formula. These variables help capture categorical effects that could further explain variations in `HP`\n",
    "- Model 5 to Model 6\n",
    "  - Model 6 uses specific indicator variables for certain categories\n",
    "  - e.g. `I(Q(\"Type 1\")==\"Normal\")` and `I(Q(\"Type 1\")==\"Water\")` are indicator variables for the `Type 1` categories `Normal` and `Water`\n",
    "  - Model 6 is more focused on specific categories\n",
    "- Model 6 to Model 7\n",
    "  - Model 7 changes the sign from `+` to `*`, which includes all interactions among these continuous variables\n",
    "  - As a result, Model 7 is more complex with interaction terms that capture how predictors interact with each other\n",
    "\n",
    "## GPT's summary\n",
    "- https://chatgpt.com/share/6736d2cc-1ff0-8006-b24e-e9364ae01960"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW7 Q9\n",
    "\n",
    "The illustration emphasizes the trade-off between model complexity and generalizability. Here's my conclusion.\n",
    "- For complex models (like Model 7)\n",
    "  - Advantages\n",
    "    - The complexity can help capture subtle relationships within the sample, leading to a high $R^2$ which is the explanatary power\n",
    "  - Disadvantages\n",
    "    - P-values can be high, suggesting that the evidence supporting certain coefficients is weak\n",
    "    - Overfitting can happen, which mean that our model captures noise rather than true underlying patterns\n",
    "    - The reuslt from the model is hard for us to interpret, leading to low interpretability\n",
    "    - As a result, it's hard to generalize this model to other new datasets\n",
    "- For simpler models (like Model 6)\n",
    "  - Advantages\n",
    "    - P-values are relatively lower\n",
    "    - It has less overfitting, which means that the model captures less noise but more true underlying patterns\n",
    "    - The reuslt from the model is easier to interpret\n",
    "    - As a result, it has a higher generalizability over different datasets\n",
    "  - Disadvantages\n",
    "    - It has a relatively low $R^2$, explanatary power, since it's simpler\n",
    "\n",
    "## GPT's summary\n",
    "- https://chatgpt.com/share/6736da9f-0508-8006-a506-8825cfea10b1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
